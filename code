import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV

# Load the dataset
df = pd.read_csv(r'C:\Users\Razvan\Desktop\proiectIRA\household_power_consumption.txt', sep=';', 
                 parse_dates={'dt': ['Date', 'Time']}, infer_datetime_format=True, 
                 low_memory=False, na_values=['nan', '?'], index_col='dt')

# Handle missing values by filling with mean
for j in range(0, 7):
    df.iloc[:, j] = df.iloc[:, j].fillna(df.iloc[:, j].mean())

# Function to convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = pd.DataFrame(data)
    cols, names = list(), list()
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    agg = pd.concat(cols, axis=1)
    agg.columns = names
    if dropnan:
        agg.dropna(inplace=True)
    return agg

# Resample the data to hourly intervals
df_resample = df.resample('h').mean()

# Scale the data
values = df_resample.values
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)

# Frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
reframed.drop(reframed.columns[[8, 9, 10, 11, 12, 13]], axis=1, inplace=True)

# Split into train and test sets
values = reframed.values
n_train_time = 365 * 24
train = values[:n_train_time, :]
test = values[n_train_time:, :]
train_x, train_y = train[:, :-1], train[:, -1]
test_x, test_y = test[:, :-1], test[:, -1]

# Split train data into train and validation sets
train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)

# Define the model
model = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8, random_state=42)

# Define early stopping
early_stopping = xgb.callback.EarlyStopping(rounds=10, metric_name='rmse', data_name='validation_1', maximize=False)

# Train the model with evaluation metrics and early stopping
eval_set = [(train_x, train_y), (val_x, val_y)]
eval_result = model.fit(train_x, train_y, eval_metric='rmse', eval_set=eval_set, verbose=True, early_stopping_rounds=10)

# Make a prediction
yhat = model.predict(test_x)

# Invert scaling for forecast
inv_yhat = np.concatenate((yhat.reshape(-1, 1), test_x[:, 1:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:, 0]

# Invert scaling for actual
inv_y = np.concatenate((test_y.reshape(-1, 1), test_x[:, 1:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:, 0]

# Calculate RMSE and R² score
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
r2 = r2_score(inv_y, inv_yhat)
print(f'Test RMSE: {rmse:.3f}')
print(f'Test R²: {r2:.3f}')

# Plotting Actual vs Predicted Power Consumption
plt.figure(figsize=(14, 7))
subset = 1000  # Number of points to plot
plt.plot(inv_y[:subset], label='Actual', alpha=0.7)
plt.plot(inv_yhat[:subset], label='Predicted', alpha=0.7)
plt.title('Actual vs Predicted Power Consumption')
plt.xlabel('Time')
plt.ylabel('Power Consumption')
plt.legend()
plt.show()

# Extract evaluation results
results = model.evals_result()
epochs = len(results['validation_0']['rmse'])
x_axis = range(0, epochs)

# Plot RMSE over epochs
plt.figure(figsize=(14, 7))
plt.plot(x_axis, results['validation_0']['rmse'], label='Train RMSE')
plt.plot(x_axis, results['validation_1']['rmse'], label='Validation RMSE')
plt.title('XGBoost RMSE over Epochs')
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)
plt.show()

# Function to calculate R² score during training
class R2ScoreCallback(xgb.callback.TrainingCallback):
    def __init__(self):
        self.history = {'train_r2': [], 'val_r2': []}
    
    def after_iteration(self, model, epoch, evals_log):
        train_pred = model.predict(xgb.DMatrix(train_x))
        val_pred = model.predict(xgb.DMatrix(val_x))
        train_r2 = r2_score(train_y, train_pred)
        val_r2 = r2_score(val_y, val_pred)
        self.history['train_r2'].append(train_r2)
        self.history['val_r2'].append(val_r2)
        return False

# Initialize the callback
r2_callback = R2ScoreCallback()

# Train the model with the R2ScoreCallback
eval_result = model.fit(train_x, train_y, eval_metric='rmse', eval_set=eval_set, verbose=False, early_stopping_rounds=10, callbacks=[r2_callback])

# Extract R² scores
history = r2_callback.history

# Plot R² over epochs
plt.figure(figsize=(14, 7))
plt.plot(x_axis, history['train_r2'], label='Train R²')
plt.plot(x_axis, history['val_r2'], label='Validation R²')
plt.title('XGBoost R² over Epochs')
plt.xlabel('Epochs')
plt.ylabel('R²')
plt.legend()
plt.grid(True)
plt.show()
